# Study Log
## Notice
1. I am sincerely grateful for the countless valuable studies that have guided my learning.  
2. This is a personal study space and may contain inaccuracies, for which I kindly ask for your understanding.
3. If you know me, please close this page right now, I‚Äôll get too shy,,üòÖ
## Contents
| Title | Thesis | Keyword | ArXiv | Code | Accept | Study |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | 
| Visual Representation Learning | Multi-Modal Large Language Models are Effective Vision Learners | LMVR | [pdf](https://openaccess.thecvf.com/content/WACV2025/papers/Sun_Multi-Modal_Large_Language_Models_are_Effective_Vision_Learners_WACV_2025_paper.pdf) | [GitHub](https://github.com/lisun-ai/LMVR) | WACV 2025 | [20250413_1956](2025/LMVR_Í≥µÎ∂Ä_20250413_1956.pdf) |
| LLaMA | LLaMA: Open and Efficient Foundation Language Models | LLM | [ArXiv](https://arxiv.org/abs/2302.13971) | [Github](https://github.com/meta-llama/llama) | ArXiv 2023 | [20250416_0112](2025/LLaMA_Í≥µÎ∂Ä_20250416_0112.pdf) |
| LLaVA | Visual Instruction Tuning | VLM | [ArXiv](https://arxiv.org/abs/2304.08485) | [Github](https://github.com/haotian-liu/LLaVA) | NeurIPS 2023 Oral | [20250418_0017](2025/LLaVAÍ≥µÎ∂Ä_20250418_0017.pdf) |
| MoAI | MoAI: Mixture of All Intelligence  for Large Language and Vision Models | LLVM | [ArXiv](https://arxiv.org/abs/2403.07508) | [GitHub](https://github.com/ByungKwanLee/MoAI) | ECCV 2024 | [20250429_0048](2025/MoAIÍ≥µÎ∂Ä_20250429_0048.pdf) |
| CoLLaVO | CoLLaVO: Crayon Large Language and Vision mOdel | LLVM | [ArXiv](https://arxiv.org/abs/2402.11248) | [GitHub](https://github.com/ByungKwanLee/CoLLaVO?tab=readme-ov-file) | ACL 2024 | [20250430_0022](2025/CollavoÍ≥µÎ∂Ä_20250430_0022.pdf) |
| Efficient Finetuning | QLORA: Efficient Finetuning of Quantized LLMs | QLORA | [ArXiv](https://arxiv.org/abs/2305.14314) | [GitHub](https://github.com/artidoro/qlora) | NeurIPS 2023 oral | [2020506_0050](2025/QLORAÍ≥µÎ∂Ä_20250506_0050.pdf) |
| Post-training Quantization | Q-VLM: Post-training Quantization for Large Vision-Language Models | Q-VLM | [ArXiv](https://arxiv.org/abs/2410.08119) | [GitHub](https://github.com/ChangyuanWang17/QVLM) | NeurIPS 2024 | [2020506_0058](2025/Q-VLMÍ≥µÎ∂Ä_20250506_0058.pdf) |
| Parameter-Efficient Continual learning | Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters | MoE-Adapters4CL | [ArXiv](https://arxiv.org/abs/2403.11549) | [GitHub](https://github.com/JiazuoYu/MoE-Adapters4CL) | CVPR 2024 | [20250516_0330](2025/MoE-Adapters4CLÍ≥µÎ∂Ä_20250516_0330.pdf) |
| Continual learning | Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models | ZSCL | [ArXiv](https://arxiv.org/abs/2303.06628) | [GitHub](https://github.com/Thunderbeee/ZSCL) | ICCV 2023 | [20250518_2345](2025/ZSCLÍ≥µÎ∂Ä_20250518_2345.pdf) |
| Phantom | Phantom of Latent for Large Language and Vision Models | LLVM | [ArXiv](https://arxiv.org/abs/2409.14713) | [GitHub](https://github.com/ByungKwanLee/Phantom) | Under Review | [20250521_0112](2025/PhantomÍ≥µÎ∂Ä_20250521_0112.pdf) |
| Parameter-Efficient Continual learning | SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning | SD-LoRA | [ArXiv](https://arxiv.org/abs/2501.13198) | [GitHub](https://github.com/WuYichen-97/SD-Lora-CL?tab=readme-ov-file) | ICLR 2025 Oral | [20250604_0122](2025/SD-LoRAÍ≥µÎ∂Ä_20250604_0122.pdf) |
| Multimodal Dialogue Response Generation | BI-MDRG: Bridging Image History in  Multimodal Dialogue Response Generation | BI-MDRG | [ArXiv](https://arxiv.org/abs/2408.05926) | [GitHub](https://github.com/hee-suk-yoon/BI-MDRG) | ECCV 2024 | [20250605_0203](2025/BI-MDRGÍ≥µÎ∂Ä_20250605_0203.pdf) |
| Language Priors Benchmark | VLind-Bench: Measuring Language Priors in Large Vision-Language Models | VLind-Bench | [ArXiv](https://arxiv.org/abs/2406.08702) | [GitHub](https://github.com/klee972/vlind-bench) | NAACL 2025 Findings | [20250613_0225](2025/VLind-BenchÍ≥µÎ∂Ä_20250613_0225.pdf) |
| Open-Vocabulary Semantic Segmentation | Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation | NACLIP | [ArXiv](https://arxiv.org/abs/2404.08181) | [GitHub](https://github.com/sinahmr/NACLIP/tree/main) | WACV 2025 | [20250616_1135](2025/NACLIPÍ≥µÎ∂Ä_20250616_1135.pdf) |
| Representation Bending | Representation Bending for Large Language Model Safety | RepBend | [ArXiv](https://arxiv.org/abs/2504.01550) | [GitHub](https://github.com/AIM-Intelligence/RepBend) | ACL 2025 | [20250619_1444](2025/RepBendÍ≥µÎ∂Ä_20250619_1444.pdf) |
| Image-Text Matching | MASS: Overcoming Language Bias in Image-Text Matching | MASS | [ArXiv](https://arxiv.org/abs/2501.11469) | No Code | AAAI 2025 | [20250621_1742](2025/MASSÍ≥µÎ∂Ä_20250621_1742.pdf) |
| Weakly Supervised Semantic Segmentation | Exploring CLIP‚Äôs Dense Knowledge for Weakly Supervised Semantic Segmentation | ExCEL | [ArXiv](https://arxiv.org/abs/2503.20826) | [GitHub](https://github.com/zwyang6/ExCEL) | CVPR 2025 | [20250625_0024](2025/ExCELÍ≥µÎ∂Ä_20250625_0024.pdf), [20250630_1308](2025/ExCEL_ÏÑùÏÇ¨ÏïåÌã∞ÏûêÎ£å_20250630_1308.pdf) |
| Referring Image Segmentation | Extending CLIP‚Äôs Image-Text Alignment to Referring Image Segmentation | RISCLIP | [ArXiv](http://arxiv.org/abs/2306.08498) | No Code | NAACL 2024 | [20250711_0156](2025/RISCLIPÍ≥µÎ∂Ä_20250711_0156.pdf) |
| Open-Vocabulary Semantic Segmentation | SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference | SCLIP | [ArXiv](https://arxiv.org/abs/2312.01597) | [GitHub](https://github.com/wangf3014/SCLIP) | ECCV 2024 | [20250719_1707](2025/SCLIPÍ≥µÎ∂Ä_20250719_1707.pdf) |
| Weakly Supervised Semantic Segmentation | Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation | WeCLIP | [ArXiv](https://arxiv.org/abs/2406.11189) | [GitHub](https://github.com/zbf1991/WeCLIP) | CVPR 2024 Highlight | [20250719_1721](2025/WeCLIPÍ≥µÎ∂Ä_20250719_1721.pdf) |
| Weakly Supervised Semantic Segmentation | Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation | SeCO | [ArXiv](https://arxiv.org/abs/2402.18467) | [GitHub](https://github.com/zwyang6/SeCo) | CVPR 2024 | [20250719_1722](2025/SeCOÍ≥µÎ∂Ä_20250719_1722.pdf) |
| Weakly Supervised Semantic Segmentation | Token Contrast for Weakly-Supervised Semantic SegmentationToken Contrast for Weakly-Supervised Semantic Segmentation | ToCo | [ArXiv](https://arxiv.org/abs/2303.01267) | [GitHub](https://github.com/rulixiang/ToCo) | CVPR 2023 | [20250719_1722](2025/ToCoÍ≥µÎ∂Ä_20250719_1722.pdf) |
| Open-Vocabulary Semantic Segmentation | ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference | ClearCLIP | [ArXiv](https://arxiv.org/abs/2407.12442) | [GitHub](https://github.com/mc-lan/ClearCLIP) | ECCV 2024 | [20250720_0106](2025/ClearCLIPÍ≥µÎ∂Ä_20250720_0106.pdf), [20250724_1431](2025/ClearCLIP_Î®∏Ïã†ÌåÄÏïåÌã∞ÏûêÎ£å_20250724_1431.pdf) |
| Open-Vocabulary Semantic Segmentation | Extract Free Dense Labels from CLIP | MaskCLIP | [ArXiv](https://arxiv.org/abs/2112.01071) | [GitHub](https://github.com/chongzhou96/MaskCLIP?tab=readme-ov-file) | ECCV 2022 Oral | [20250720_1722](2025/MaskCLIPÍ≥µÎ∂Ä_20250720_1722.pdf) |
| Open-Vocabulary Semantic Segmentation | ProxyCLIP: Proxy Attention Improves CLIP  for Open-Vocabulary Segmentation | ProxyCLIP | [ArXiv](https://arxiv.org/abs/2408.04883) | [GitHub](https://github.com/mc-lan/ProxyCLIP?tab=readme-ov-file) | ECCV 2024 | [20250722_0105](2025/ProxyCLIPÍ≥µÎ∂Ä_20250722_0105.pdf) |
| Retrieval-Augmented Generation | VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents | VDocRAG | [ArXiv](https://arxiv.org/abs/2504.09795) | [GitHub](https://github.com/nttmdlab-nlp/VDocRAG) | CVPR 2025 | [20250811_2141](2025/VDocRAGÍ≥µÎ∂Ä_20250811_2141.pdf) |
| Open-Vocabulary Object Detection | Simple Open-Vocabulary Object Detection with Vision Transformers | OWL-VIT v1 | [ArXiv](https://arxiv.org/abs/2205.06230) | [GitHub](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit) | ECCV 2022 | [20250812_2243](2025/OWL-VITÍ≥µÎ∂Ä_20250812_2243.pdf) |
| Open-Vocabulary Object Detection | Scaling Open-Vocabulary Object Detection | OWL-VIT v2 | [ArXiv](https://arxiv.org/abs/2306.09683) | [GitHub](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit) | NeurIPS 2023 | [20250812_2243](2025/OWL-VITÍ≥µÎ∂Ä_20250812_2243.pdf) |
| Open-Vocabulary Semantic Segmentation | Effective SAM Combination for Open-Vocabulary Semantic Segmentation | ESC-Net | [ArXiv](https://arxiv.org/abs/2411.14723) | No Code | CVPR 2025 | [20250821_2117](2025/ESCNet_Î®∏Ïã†ÌåÄÏïåÌã∞ÏûêÎ£å_20250821_2117.pdf) |
