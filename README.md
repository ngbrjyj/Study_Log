# Model
| Model | Thesis | Keyword | ArXiv | Code | Accept | Study |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | 
| LLaMA | LLaMA: Open and Efficient Foundation Language Models | LLM | [ArXiv](https://arxiv.org/abs/2302.13971) | [Github](https://github.com/meta-llama/llama) | ArXiv 2023 | [20250416_0112](Model/LLaMA_공부_20250416_0112.pdf) |
| LLaVA | Visual Instruction Tuning | VLM | [ArXiv](https://arxiv.org/abs/2304.08485) | [Github](https://github.com/haotian-liu/LLaVA) | NeurIPS 2023 Oral | [20250418_0017](Model/LLaVA_공부_20250418_0017.pdf) |
| MoAI | MoAI: Mixture of All Intelligence  for Large Language and Vision Models | LLVM | [ArXiv](https://arxiv.org/abs/2403.07508) | [GitHub](https://github.com/ByungKwanLee/MoAI) | ECCV 2024 | [20250429_0048](Model/MoAI공부_20250429_0048.pdf) |
| CoLLaVO | CoLLaVO: Crayon Large Language and Vision mOdel | LLVM | [ArXiv](https://arxiv.org/abs/2402.11248) | [GitHub](https://github.com/ByungKwanLee/CoLLaVO?tab=readme-ov-file) | ACL 2024 | [20250430_0022](Model/Collavo공부_20250430_0022.pdf) |
| Phantom | Phantom of Latent for Large Language and Vision Models | LLVM | [ArXiv](https://arxiv.org/abs/2409.14713) | [GitHub](https://github.com/ByungKwanLee/Phantom) | Under Review | [20250521_0112](Model/Phantom공부_20250521_0112.pdf) |

# Task
| Task | Thesis | Keyword | ArXiv | Code | Accept | Study |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | 
| Visual Representation Learning | Multi-Modal Large Language Models are Effective Vision Learners | LMVR | [pdf](https://openaccess.thecvf.com/content/WACV2025/papers/Sun_Multi-Modal_Large_Language_Models_are_Effective_Vision_Learners_WACV_2025_paper.pdf) | [GitHub](https://github.com/lisun-ai/LMVR) | WACV 2025 | [20250413_1956](Task/LMVR_공부_20250413_1956.pdf) |
| Efficient Finetuning | QLORA: Efficient Finetuning of Quantized LLMs | QLORA | [ArXiv](https://arxiv.org/abs/2305.14314) | [GitHub](https://github.com/artidoro/qlora) | NeurIPS 2023 oral | [2020506_0050](Task/QLORA공부_20250506_0050.pdf) |
| Post-training Quantization | Q-VLM: Post-training Quantization for Large Vision-Language Models | Q-VLM | [ArXiv](https://arxiv.org/abs/2410.08119) | [GitHub](https://github.com/ChangyuanWang17/QVLM) | NeurIPS 2024 | [2020506_0058](Task/Q-VLM공부_20250506_0058.pdf) |
| Parameter-Efficient Continual learning | Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters | MoE-Adapters4CL | [ArXiv](https://arxiv.org/abs/2403.11549) | [GitHub](https://github.com/JiazuoYu/MoE-Adapters4CL) | CVPR 2024 | [20250516_0330](Task/MoE-Adapters4CL공부_20250516_0330.pdf) |
| Continual learning | Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models | ZSCL | [ArXiv](https://arxiv.org/abs/2303.06628) | [GitHub](https://github.com/Thunderbeee/ZSCL) | ICCV 2023 | [20250518_2345](Task/ZSCL공부_20250518_2345.pdf) |
| Parameter-Efficient Continual learning | SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning | SD-LoRA | [ArXiv](https://arxiv.org/abs/2501.13198) | [GitHub](https://github.com/WuYichen-97/SD-Lora-CL?tab=readme-ov-file) | ICLR 2025 Oral | [20250604_0122](Task/SD-LoRA공부_20250604_0122.pdf) |
| Multimodal Dialogue Response Generation | BI-MDRG: Bridging Image History in  Multimodal Dialogue Response Generation | BI-MDRG | [ArXiv](https://arxiv.org/abs/2408.05926) | [GitHub](https://github.com/hee-suk-yoon/BI-MDRG) | ECCV 2024 | [20250605_0203](Task/BI-MDRG공부_20250605_0203.pdf) |
| Language Priors Benchmark | VLind-Bench: Measuring Language Priors in Large Vision-Language Models | VLind-Bench | [ArXiv](https://arxiv.org/abs/2406.08702) | [GitHub](https://github.com/klee972/vlind-bench) | NAACL 2025 Findings | [20250610_2354](Task/VLind-Bench공부_20250610_2354.pdf) |
