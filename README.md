# Model
| Model | Thesis | ArXiv | Code | Accept | Study |
| :---: | :---: | :---: | :---: | :---: | :---: | 
| LLaMA | LLaMA: Open and Efficient Foundation Language Models | [ArXiv](https://arxiv.org/abs/2302.13971) | [Github](https://github.com/meta-llama/llama) | ArXiv 2023 | [20250416_0112](Model/LLaMA_공부_20250416_0112.pdf) |
| LLaVA | Visual Instruction Tuning | [ArXiv](https://arxiv.org/abs/2304.08485) | [Github](https://github.com/haotian-liu/LLaVA) | NeurIPS 2023 Oral | [20250418_0017](Model/LLaVA_공부_20250418_0017.pdf) |


# Task
| Task | Thesis | ArXiv | Code | Accept | Study |
| :---: | :---: | :---: | :---: | :---: | :---: | 
| Visual Representation Learning | Multi-Modal Large Language Models are Effective Vision Learners (LMVR) | [pdf](https://openaccess.thecvf.com/content/WACV2025/papers/Sun_Multi-Modal_Large_Language_Models_are_Effective_Vision_Learners_WACV_2025_paper.pdf) | [GitHub](https://github.com/lisun-ai/LMVR) | WACV 2025 | [20250413_1956](Task/LMVR_공부_20250413_1956.pdf) |
